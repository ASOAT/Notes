Authors: 剑 孙, 赫 张, 晓聪 赵, 懿如 刘, 野 田
Year: 2024
Status: #literature
Tags: 
# 面向自动驾驶测试的交互场景策略建模与仿真
把场景生成转换成交互决策建模问题。
### 基于博弈的交互策略建模
策略性交互行为建模满足以下要求: 
(1) 行为模型不仅体现个体级的行为动因, 还体现个体间动作依赖关系
	*- 如何建模关系（因果网络，博弈模型）*
	*- 利用该关系能够扰动整个交互网络以制造挑战场景吗？*
(2) 行为模型中包含驾驶行为社会性的特征参数, 以表征背景车在社会性层面的异质性。
#### 效用函数
假设规划得到的是一条包含 $N$ 个轨迹段的无碰撞轨迹，可以分为 $p_{i}^1, \dots, p_{i}^n$ 个点
**个体收益项**：
$$
R_I(p_{i} = \tau_{i}(p_{i}^N) - \alpha \sum_{n = 1}^N ||r(p_{i}^n) - p_{i}^n||
$$
其中 $\tau_{i}$ 表示轨迹在车道中心线上的投影长度，这一项表示行驶效率；
$r(p_{i}^n)$ 表示在车道中心线上的投影点，第二项表示轨迹的偏离程度。

**群体收益项**：
$$
R_G(p_{i},p_{-i}) = (N - n_{m} + 1) \cdot \Vert p_{i}^{n_{m}} - p_{-i}^{n_{m}} \Vert
$$
其中 $n_{m}$ 为交互双方距离最近时的轨迹点序号，因此这两项分别表征了冲突时的时间和空间的紧急程度。
*关于时间的紧急程度有待修正，目前来看并不是很能说明问题*

**效用函数**
$$
U = \cos \theta \cdot R_{I} + \sin \theta \cdot R_{G}
$$
其中 $\theta$ 为文章新定义的个体社会交互值，范围在 $\left[- \frac{\pi}{2}, \frac{\pi}{2} \right]$，能够表示不同个体的社会性。当 $\theta=0$ 时表示个体完全自私。
#### 动作依赖关系
决策目标为效用最大化的轨迹：
$$
p_i^* = \arg \max_{x_{i \in \mathbb{P}_{i}}}U(p_{i},p_{-i},\theta_{i})
$$
车辆受到运动学的限制和道路边界的限制。
由于交互对象的决策结果 $p_{-i}$ 未知，因此优化问题无法直接求解，引入博弈模型中的另一假设：**博弈双方均了解对方的决策遵循"效用最大化"**, 由此可以将交互对象的决策结果表述为：
$$
p_{-i}^*\left(p_i\right)=\arg\max_{x_{-i}\in\mathbb{P}_{-i}}U_{-i}\left(p_{-i},p_i,\hat{\theta}_{-i}\right)
$$
形成一个双层优化问题，使用最优回应法（iterative best response）求解。

为确保实时性，需要对博弈模型线性化转化。
#### 交互倾向值
- 为了推测交互对象的实际 IPV，在交互值范围内采样 K 个样本并分别构建虚拟交互对象。
- 通过轨迹对比计算（*这里还是在默认双方遵循效用最大化*）各个虚拟交互对象的似然值$$
l_k(\theta_{-i,k},\theta_{-i})\propto p(\boldsymbol{x}_k|\theta_{-i})\propto\mathcal{N}(\boldsymbol{x}_k|\bar{\boldsymbol{x}}\left(\theta_{-i}\right),\sigma^2)
$$
- 通过归一化的似然值加权估计实际的 IPV。


---
# Extension Papers
[[迭代最优回应法]]