
在这些文件的基础上，完成 game_solver，目的是模拟正常的多车的驾驶场景，要求：
- 分为 base_strategy：碰撞约束(硬约束)；车道边界约束（硬约束，如果车道边界之间存在细小的狭缝则可以认为该区域是可以跨越的，不需要约束. 这些约束都要应用到下方的主车和背景车中；
- leader_strategy：使用 MPC 求解当前场景下的最优路径，该部分中需要包含对其他车辆的轨迹预测，并使用预测的结果来进行规划。主车的规划目的是尽可能在参考轨迹上行驶并到达参考轨迹的终点；利用 casadi 求解最优解
- followers_strategy：其他所有背景车的策略。使用势博弈，效用函数满足
   $$
R = R_{\text{self}} + R_{\text{coop}} + R_{\text{chaos}} - \text{Penalties}
$$

	- **自我中心奖励** $R_{\text{self}} = -\alpha \cdot$ 路径偏离 $+$ 速度奖励（规划得到的路径在参考轨迹上的投影长度）; 路径偏离即自身位置到在参考轨迹上的投影点的距离
	- **协作奖励** $R_{\text{coop}} = \beta \cdot \sum e^{- d_{ij} / \sigma}$（导致主车的动作被扰乱或轨迹被干扰，可以使用主车的路径偏离量）另一方面则是背景车辆之间的协作，背景车辆和背景车辆之间考虑不导致对方偏离路径
	- **混乱激励** $R_{\text{chaos}} = (1 - \alpha - \beta) \cdot \tanh \left ( \sum | \cos \theta_{ij} | \right)$（整个场景的混乱程度）
	- **惩罚项**：碰撞惩罚、越界惩罚、低速惩罚等
	- 具体的奖励系数和车辆的 alpha 以及 beta 系数有关，表现不同的车辆特征
	- 博弈的求解使用更高效的方法，alpha 表现为自私，beta 表现为考虑群体收益；
	- 利用 casadi 或 MCTS 求解所有背景车辆的动作，动作空间包括加速度，转向角以及生成的时间和生成的车道以及生成的初始速度
	- 不需要为背景车辆添加速度惩罚，背景车辆之间互相让行是允许的的。
- select_leader: 选择主车的函数。在整个场景中选择一辆主车，其他所有的车辆的目的就是干扰这个主车。选择的标准为：主车即将与一定范围内的多车进行交互，（此处需要预测其他车辆的轨迹（或者直接使用规划得到的轨迹代替预测，假设规划的路径就是预测得到，当然为了避免规划的跳动还需要对其进行平均化等处理））
- 修改车辆生成的逻辑，把它单独从 main 中独立出来形成一个文件，将车辆生成的时间也一起加入博弈的动作空间，目标在于使生成的车辆能够及时加入交互中，因此这一层博弈的预测时域需要长一点；